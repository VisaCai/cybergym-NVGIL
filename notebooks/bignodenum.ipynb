{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(\"/home/kalic/Desktop/AI_P/CyberBattleSim\")\n",
    "from cyberbattle.simulation import model\n",
    "from cyberbattle.samples.topology_nodeinfo import topology_and_nodeinfo_env\n",
    "import gym\n",
    "import cyberbattle.agents.baseline.agent_wrapper as w\n",
    "import logging\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "import cyberbattle.agents.baseline.plotting as p\n",
    "import cyberbattle.agents.baseline.agent_randomcredlookup as rca\n",
    "import cyberbattle.agents.baseline.agent_tabularqlearning as tqa\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "from cyberbattle.agents.baseline.agent_wrapper import Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_size = None\n",
    "iteration_count = 300\n",
    "training_episode_count = 10\n",
    "eval_episode_count = 2\n",
    "maximum_node_count = 10\n",
    "maximum_total_credentials = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 d\n",
      "why\n",
      "7 s\n"
     ]
    }
   ],
   "source": [
    "gymid = \"topology_and_nodeinfo_env-v0\"\n",
    "# size_N, size_NGN, dis_type, network_cc, connect_percent\n",
    "gym_env = gym.make(gymid, size_N = 7, size_NGN = 2, dis_type = \"gradient_up\", network_cc = \"chained\", connect_percent = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_node_count=maximum_node_count,\n",
    "    maximum_total_credentials=maximum_total_credentials,\n",
    "    identifiers=gym_env.identifiers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### DQL\n",
      "Learning with: episode_count=10,iteration_count=300,ϵ=0.9,ϵ_min=0.1, ϵ_expdecay=5000,γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalic/Desktop/AI_P/CyberBattleSim/cyberbattle/agents/baseline/agent_dql.py:382: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  state_batch = torch.tensor(states_to_consider).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 1 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 1/96 (0.01)\n",
      "    explore-remote: 2/194 (0.01)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/4 (0.00)\n",
      "    exploit-remote: 0/3 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 21\n",
      "  Episode 2 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 1/91 (0.01)\n",
      "    explore-remote: 2/186 (0.01)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/10 (0.00)\n",
      "    exploit-remote: 0/10 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 33\n",
      "  Episode 3 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 1/85 (0.01)\n",
      "    explore-remote: 2/148 (0.01)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/31 (0.00)\n",
      "    exploit-remote: 0/33 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 13\n",
      "  Episode 4 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/75 (0.00)\n",
      "    explore-remote: 2/168 (0.01)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 1/4 (0.20)\n",
      "    exploit-remote: 0/50 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 9\n",
      "  Episode 5 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/72 (0.00)\n",
      "    explore-remote: 2/147 (0.01)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 1/7 (0.12)\n",
      "    exploit-remote: 0/71 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 10\n",
      "  Episode 6 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/73 (0.00)\n",
      "    explore-remote: 2/123 (0.02)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 1/3 (0.25)\n",
      "    exploit-remote: 0/98 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Evaluate the Deep Q-learning agent\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m my_dql_run \u001b[39m=\u001b[39m learner\u001b[39m.\u001b[39;49mepsilon_greedy_search(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     cyberbattle_gym_env\u001b[39m=\u001b[39;49mgym_env,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     environment_properties\u001b[39m=\u001b[39;49mep,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     learner\u001b[39m=\u001b[39;49mdqla\u001b[39m.\u001b[39;49mDeepQLearnerPolicy(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         ep\u001b[39m=\u001b[39;49mep,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         gamma\u001b[39m=\u001b[39;49m\u001b[39m0.015\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         replay_memory_size\u001b[39m=\u001b[39;49m\u001b[39m10000\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         target_update\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         batch_size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         \u001b[39m# torch default learning rate is 1e-2\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m# a large value helps converge in less episodes\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     episode_count\u001b[39m=\u001b[39;49mtraining_episode_count,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     iteration_count\u001b[39m=\u001b[39;49miteration_count,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     epsilon\u001b[39m=\u001b[39;49m\u001b[39m0.90\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     epsilon_exponential_decay\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     epsilon_minimum\u001b[39m=\u001b[39;49m\u001b[39m0.10\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     verbosity\u001b[39m=\u001b[39;49mVerbosity\u001b[39m.\u001b[39;49mQuiet,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     render\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     plot_episodes_length\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     title\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mDQL\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kalic/Desktop/AI_P/CyberBattleSim/notebooks/newtest.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/AI_P/CyberBattleSim/cyberbattle/agents/baseline/learner.py:287\u001b[0m, in \u001b[0;36mepsilon_greedy_search\u001b[0;34m(cyberbattle_gym_env, environment_properties, learner, title, episode_count, iteration_count, epsilon, epsilon_minimum, epsilon_multdecay, epsilon_exponential_decay, render, render_last_episode_rewards_to, verbosity, plot_episodes_length)\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    285\u001b[0m     stats[action_type][outcome][\u001b[39m'\u001b[39m\u001b[39mconnect\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 287\u001b[0m learner\u001b[39m.\u001b[39mon_step(wrapped_env, observation, reward, done, info, action_metadata)\n\u001b[1;32m    288\u001b[0m \u001b[39massert\u001b[39;00m np\u001b[39m.\u001b[39mshape(reward) \u001b[39m==\u001b[39m ()\n\u001b[1;32m    290\u001b[0m all_rewards\u001b[39m.\u001b[39mappend(reward)\n",
      "File \u001b[0;32m~/Desktop/AI_P/CyberBattleSim/cyberbattle/agents/baseline/agent_dql.py:362\u001b[0m, in \u001b[0;36mDeepQLearnerPolicy.on_step\u001b[0;34m(self, wrapped_env, observation, reward, done, info, action_metadata)\u001b[0m\n\u001b[1;32m    358\u001b[0m next_actor_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstateaction_model\u001b[39m.\u001b[39mnode_specific_features\u001b[39m.\u001b[39mget(\n\u001b[1;32m    359\u001b[0m     agent_state, action_metadata\u001b[39m.\u001b[39mactor_node)\n\u001b[1;32m    360\u001b[0m next_actor_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_actor_state_vector(next_global_state, next_actor_features)\n\u001b[0;32m--> 362\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_q_function(reward,\n\u001b[1;32m    363\u001b[0m                        actor_state\u001b[39m=\u001b[39;49maction_metadata\u001b[39m.\u001b[39;49mactor_state,\n\u001b[1;32m    364\u001b[0m                        abstract_action\u001b[39m=\u001b[39;49maction_metadata\u001b[39m.\u001b[39;49mabstract_action,\n\u001b[1;32m    365\u001b[0m                        next_actor_state\u001b[39m=\u001b[39;49mnext_actor_state)\n",
      "File \u001b[0;32m~/Desktop/AI_P/CyberBattleSim/cyberbattle/agents/baseline/agent_dql.py:346\u001b[0m, in \u001b[0;36mDeepQLearnerPolicy.update_q_function\u001b[0;34m(self, reward, actor_state, abstract_action, next_actor_state)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mpush(current_state_tensor, action_tensor, next_state_tensor, reward_tensor)\n\u001b[1;32m    345\u001b[0m \u001b[39m# optimize the target network\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimize_model()\n",
      "File \u001b[0;32m~/Desktop/AI_P/CyberBattleSim/cyberbattle/agents/baseline/agent_dql.py:295\u001b[0m, in \u001b[0;36mDeepQLearnerPolicy.optimize_model\u001b[0;34m(self, norm_clipping)\u001b[0m\n\u001b[1;32m    289\u001b[0m reward_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(batch\u001b[39m.\u001b[39mreward)\n\u001b[1;32m    291\u001b[0m \u001b[39m# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[39m# columns of actions taken. These are the actions which would've been taken\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[39m# for each batch state according to policy_net\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39m# print(f'state_batch={state_batch.shape} input={len(self.stateaction_model.state_space.dim_sizes)}')\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net(state_batch)\n\u001b[1;32m    297\u001b[0m \u001b[39m# print(f'output={output.shape} batch.action={transitions[0].action.shape} action_batch={action_batch.shape}')\u001b[39;00m\n\u001b[1;32m    298\u001b[0m state_action_values \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, action_batch)\n",
      "File \u001b[0;32m~/Desktop/pyvirtual_08/virtualforcyber/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/AI_P/CyberBattleSim/cyberbattle/agents/baseline/agent_dql.py:183\u001b[0m, in \u001b[0;36mDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    181\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layer1(x))\n\u001b[1;32m    182\u001b[0m \u001b[39m# x = F.dropout(x, p=0.5, training=self.training)\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhidden_layer2(x))\n\u001b[1;32m    184\u001b[0m \u001b[39m# x = F.dropout(x, p=0.5, training=self.training)\u001b[39;00m\n\u001b[1;32m    185\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_layer3(x))\n",
      "File \u001b[0;32m~/Desktop/pyvirtual_08/virtualforcyber/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/pyvirtual_08/virtualforcyber/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate the Deep Q-learning agent\n",
    "my_dql_run = learner.epsilon_greedy_search(\n",
    "    cyberbattle_gym_env=gym_env,\n",
    "    environment_properties=ep,\n",
    "    learner=dqla.DeepQLearnerPolicy(\n",
    "        ep=ep,\n",
    "        gamma=0.015,\n",
    "        replay_memory_size=10000,\n",
    "        target_update=10,\n",
    "        batch_size=512,\n",
    "        # torch default learning rate is 1e-2\n",
    "        # a large value helps converge in less episodes\n",
    "        learning_rate=0.01\n",
    "    ),\n",
    "    episode_count=training_episode_count,\n",
    "    iteration_count=iteration_count,\n",
    "    epsilon=0.90,\n",
    "    epsilon_exponential_decay=5000,\n",
    "    epsilon_minimum=0.10,\n",
    "    verbosity=Verbosity.Quiet,\n",
    "    render=False,\n",
    "    plot_episodes_length=False,\n",
    "    title=\"DQL\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "DQN_steps_N = []\n",
    "DQN_rewards_N = []\n",
    "my_steps_N = []\n",
    "my_rewards_N = []\n",
    "for i in range(N):\n",
    "    # Evaluate the Deep Q-learning agent\n",
    "    gym_env = gym.make(gymid, size_N = 7, size_NGN = 2, dis_type = \"gradient_up\", network_cc = \"chained\", connect_percent = 0.9)\n",
    "    dql_run = learner.epsilon_greedy_search(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=dqla.DeepQLearnerPolicy(\n",
    "            ep=ep,\n",
    "            gamma=0.015,\n",
    "            replay_memory_size=10000,\n",
    "            target_update=10,\n",
    "            batch_size=512,\n",
    "            # torch default learning rate is 1e-2\n",
    "            # a large value helps converge in less episodes\n",
    "            learning_rate=0.01\n",
    "        ),\n",
    "        episode_count=training_episode_count,\n",
    "        iteration_count=iteration_count,\n",
    "        epsilon=0.90,\n",
    "        epsilon_exponential_decay=5000,\n",
    "        epsilon_minimum=0.10,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        render=False,\n",
    "        plot_episodes_length=False,\n",
    "        title=\"DQL\"\n",
    "    )\n",
    "    my_dql_run = learner.epsilon_greedy_search(\n",
    "        cyberbattle_gym_env=gym_env,\n",
    "        environment_properties=ep,\n",
    "        learner=my_dql_run['learner'](\n",
    "        ep=ep,\n",
    "        gamma=0.015,\n",
    "        replay_memory_size=10000,\n",
    "        target_update=10,\n",
    "        batch_size=512,\n",
    "        # torch default learning rate is 1e-2\n",
    "        # a large value helps converge in less episodes\n",
    "        learning_rate=0.01\n",
    "        ),\n",
    "        episode_count=training_episode_count,\n",
    "        iteration_count=iteration_count,\n",
    "        epsilon=0.90,\n",
    "        epsilon_exponential_decay=5000,\n",
    "        epsilon_minimum=0.10,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        render=False,\n",
    "        plot_episodes_length=False,\n",
    "        title=\"DQL\"\n",
    "    )\n",
    "    DQN_steps_N.append(my_dql_run['all_steps'])\n",
    "    DQN_rewards_N.append(my_dql_run['all_episodes_rewards'])\n",
    "    my_steps_N.append(dql_run['all_steps'])\n",
    "    my_rewards_N.append(dql_run['all_episodes_rewards'])\n",
    "    print(i, my_dql_run['all_steps'], my_dql_run['all_episodes_rewards'])\n",
    "    print(i, dql_run['all_steps'], dql_run['all_episodes_rewards'])\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### DQL\n",
      "Learning with: episode_count=10,iteration_count=300,ϵ=0.9,ϵ_min=0.1, ϵ_expdecay=5000,γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kalic/Desktop/AI_P/CyberBattleSim/cyberbattle/agents/baseline/agent_dql.py:382: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  state_batch = torch.tensor(states_to_consider).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Episode 1 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/83 (0.00)\n",
      "    explore-remote: 5/207 (0.02)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 1/4 (0.20)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 32\n",
      "  Episode 2 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/86 (0.00)\n",
      "    explore-remote: 6/166 (0.03)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/42 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 11\n",
      "  Episode 3 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/77 (0.00)\n",
      "    explore-remote: 5/159 (0.03)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 1/58 (0.02)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 4\n",
      "  Episode 4 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/64 (0.00)\n",
      "    explore-remote: 6/161 (0.04)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/69 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 2\n",
      "  Episode 5 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/71 (0.00)\n",
      "    explore-remote: 2/156 (0.01)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 4/67 (0.06)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 7\n",
      "  Episode 6 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/62 (0.00)\n",
      "    explore-remote: 3/141 (0.02)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/2 (0.00)\n",
      "    exploit-remote: 3/89 (0.03)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 4\n",
      "  Episode 7 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/62 (0.00)\n",
      "    explore-remote: 1/136 (0.01)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/1 (0.00)\n",
      "    exploit-remote: 5/95 (0.05)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 4\n",
      "  Episode 8 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/53 (0.00)\n",
      "    explore-remote: 3/124 (0.02)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/3 (0.00)\n",
      "    exploit-remote: 3/114 (0.03)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 8\n",
      "  Episode 9 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/54 (0.00)\n",
      "    explore-remote: 5/129 (0.04)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/2 (0.00)\n",
      "    exploit-remote: 1/109 (0.01)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 3\n",
      "  Episode 10 stopped at t=300 \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 0/49 (0.00)\n",
      "    explore-remote: 4/122 (0.03)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 2/123 (0.02)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 4\n",
      "simulation ended\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Deep Q-learning agent\n",
    "dql_run = learner.epsilon_greedy_search(\n",
    "    cyberbattle_gym_env=gym_env,\n",
    "    environment_properties=ep,\n",
    "    learner=dqla.DeepQLearnerPolicy(\n",
    "        ep=ep,\n",
    "        gamma=0.015,\n",
    "        replay_memory_size=10000,\n",
    "        target_update=10,\n",
    "        batch_size=512,\n",
    "        # torch default learning rate is 1e-2\n",
    "        # a large value helps converge in less episodes\n",
    "        learning_rate=0.01\n",
    "    ),\n",
    "    episode_count=training_episode_count,\n",
    "    iteration_count=iteration_count,\n",
    "    epsilon=0.90,\n",
    "    epsilon_exponential_decay=5000,\n",
    "    epsilon_minimum=0.10,\n",
    "    verbosity=Verbosity.Quiet,\n",
    "    render=False,\n",
    "    plot_episodes_length=False,\n",
    "    title=\"DQL\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[105, 73, 53, 32, 51, 77, 32, 53, 24, 23] [162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0, 162.0]\n",
      "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]] <cyberbattle.agents.baseline.agent_dql.DeepQLearnerPolicy object at 0x7f8023764160>\n"
     ]
    }
   ],
   "source": [
    "print(dql_run['all_steps'],  dql_run['all_episodes_rewards'])\n",
    "print(dql_run['all_episodes_availability'],dql_run['learner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 d\n",
      "why\n",
      "7 s\n"
     ]
    }
   ],
   "source": [
    "gymid = \"topology_and_nodeinfo_env-v0\"\n",
    "# size_N, size_NGN, dis_type, network_cc, connect_percent\n",
    "gym_env2 = gym.make(gymid, size_N = 7, size_NGN = 2, dis_type = \"gradient_up\", network_cc = \"chained\", connect_percent = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Deep Q-learning agent\n",
    "dql_run2 = learner.epsilon_greedy_search(\n",
    "    cyberbattle_gym_env=gym_env,\n",
    "    environment_properties=ep,\n",
    "    learner=dql_run['learner'](\n",
    "        ep=ep,\n",
    "        gamma=0.015,\n",
    "        replay_memory_size=10000,\n",
    "        target_update=10,\n",
    "        batch_size=512,\n",
    "        # torch default learning rate is 1e-2\n",
    "        # a large value helps converge in less episodes\n",
    "        learning_rate=0.01\n",
    "    ),\n",
    "    episode_count=training_episode_count,\n",
    "    iteration_count=iteration_count,\n",
    "    epsilon=0.90,\n",
    "    epsilon_exponential_decay=5000,\n",
    "    epsilon_minimum=0.10,\n",
    "    verbosity=Verbosity.Quiet,\n",
    "    render=False,\n",
    "    plot_episodes_length=False,\n",
    "    title=\"DQL\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(training_episode_count)\n",
    "steps = dql_run['all_steps']\n",
    "y = np.mean(steps,axis=0); ystd = np.std(steps,axis=0)\n",
    "plt.plot(x,y,label='p=0.1')\n",
    "plt.fill_between(x,y-ystd,y+ystd,alpha=.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtualforcyber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
